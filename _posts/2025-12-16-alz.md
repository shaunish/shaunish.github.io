---
layout: post
title:  Using PCA to Find the Best Classifier on Genomic Data to Predict Alzheimer's Risk
categories: [PCA,Data Analytics,Python,Classification,Prediction,Dimensionality Reduction]
excerpt: Working with horrible, unwieldy datasets.
---


[Alzheimer’s disease](https://en.wikipedia.org/wiki/Alzheimer%27s_disease) is a neurodegenerative disease characterized by a buildup of malformed proteins known as amyloid plaque in the brain. It's the leading cause of dementia, and it affects around 50 million people worldwide, mostly those over the age of 65. The causes of Alzheimer’s disease are not well understood, although both environmental and genetic factors are thought to play a role. Diagnosis today is based on family history as well as cognitive testing, supplemented by medical imaging and blood tests. There are no effective treatments to slow or stop the onset of Alzheimer’s disease today, although some treatments alleviate symptoms. 

At the same time, the [cost of human genome sequencing](https://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost) has dropped dramatically, from around $100 million in 2001, when the Human Genome Project was first completed, to less than $1000 today. As a result, direct genetic testing is likely to play a larger role in predicting risk and calibrating medical diagnoses for patients. Medical professionals can use genetic testing to catch and treat diseases earlier, improving outcomes for patients. 

The purpose of this project is to build a classifier model from genetic data to help predict Alzheimer’s disease risk in patients. Alzheimer’s disease is around 70% heritable, pointing to a large genetic component; yet the genetic markers of the disease are highly [polygenic and complex](https://pmc.ncbi.nlm.nih.gov/articles/PMC10024184/). Although early diagnosis today may not have a strong effect on patient outcome, as effective treatments are still being researched, a robust genetic prediction model can help narrow down the true causes of the disease. This is also a highly personal project for me - my grandfather had Alzheimer's disease.

The data source for this project will be [this publicly available dataset](https://www.kaggle.com/datasets/fereshteeslami/alzheimer-gene-expression-dataset). It contains gene expression data from 22 Alzheimer’s disease patients at various levels of severity of disease, as well as 9 healthy control subjects. This dataset does not contain the full genome of these patients, but a subset of genes whose expressions showed high correlation with the results of a cognitive test ([the MMSE](https://cgatoolkit.ca/Uploads/ContentDocuments/MMSE.pdf)) as well as neurofibrillary tangle score (a physical marker in the brain that is a characteristic of Alzheimer’s disease). Even though the dataset only includes this subset of correlated genes, the dataset still contains 13,147 genes(!), meaning that dimensionality reduction will be an important part of this analysis.

The goal of the project is to create a classifier model that can accurately use genomic data to predict whether a patient is likely to be in one of these categories: Severe Alzheimer’s, Moderate Alzheimer’s, Incipient (Mild) Alzheimer’s, or Control. In addition, because of the small sample size of the dataset, we'll also train the classifier models on a simplified version of the data in which the patients are classified as either Patient or Control. An initial hypothesis: due to the complexity of genomic data, non-linear or non-parametric classifiers will perform better than linear or parametric classifiers. 

In order to test this hypothesis, we'll take the following steps:

1. **Pre-process and scale the data.** The data is read into a pandas DataFrame and scaled to prepare for dimensionality reduction. Some values for some genes are missing. These will be filled in with the mean value of the marker among all samples.
2. **Split the data into chunks for cross-validation.** The data is randomly split into three chunks. Each classifier will use two chunks as training data and the remaining chunk as testing data. This process will then repeat using a different chunk as testing data until all three chunks have been used as testing data, and the final classifier score will be the average of the three runs.
3. **Perform dimensionality reduction on the training data using PCA.** This has two benefits- not only is the number of dimensions reduced, but genes are highly correlated. PCA removes this correlation. One parameter that may have a large effect on the accuracy of the classifiers is the number of components used in PCA. With such a small dataset, there is a danger of overfitting if too many components are used. We'll use an elbow plot to choose the number of components.
4. **Use the transformed training data to train several classifiers.** The classifiers to be tested include the following: Logistic Regression, K Nearest Neighbors (n=3), Linear SVM, RBF Kernel SVM, Decision Tree, Random Forest(80 estimators), Neural Net, AdaBoost, and
Gaussian Naive Bayes. Unless specified, parameters are chosen to be sklearn defaults.
5. **Project the testing data onto the PCA space, and measure the accuracy of each classifier on the testing data.**
6. **Repeat Steps 3-5 above for each split of the data.** The final cross-validation score will be the mean of the three scores.
The classifier that performs the best on average over all iterations will be chosen as the best classifier for this data.

### Part 1: Dimensionality Reduction Using PCA

First, we'll do a PCA transformation of the data. Typically, when doing data analysis, we want a much higher number of samples vs. features in our data. Most models don't even work if you have more features than samples - the resulting covariance matrix is not invertible, so the optimization algorithm underlying techniques like regression gets stuck. In this case, we have 13,147 features and just 31 samples. But we can still work with this data using dimensionality reduction techniques like PCA, which projects the original data into another space, carefully chosen such that each dimension is along the axis of the maximum variance of the original data. Then we can just pick the first few dimensions of the projection, and throw the rest away without losing too much. That way, we can reduce the number of features from 13,147 to a more manageable number, and feed the result into our classifier. For this project, I'm using the sklearn PCA library to do the transformation.

![First two principal components of the data vs. Alzheimer's severity](/images/alz/alz_pca.png)

Figure 1 plots the severity of Alzheimer’s incidence vs. the first two principal components of the data after PCA transformation. Note that the control samples (denoted by the lightest color and labeled 0 in the legend) are not linearly separable from the rest of the data. We have to determine the right number of features to include, and we can use an elbow plot to decide. In the plot (Figure 2), we can see that there is an inflection point after three components, so let's choose to be the best number of components to include in training the classifiers. Although the total variance explained in the first three components is only 0.389871 of the original data, including a larger number of components risks overfitting, especially when the number of samples is so small. Three components is a balance between capturing as much of the underlying data structure as possible and overfitting.

![PCA Elbow Plot](/images/alz/elbow.png)

### Part 2: Full Multi-Label Classification

My hypothesis: non-linear, non-parametric classifiers will perform better than linear, parametric classifiers due to the complexity of the data. Due to the random nature of the data split, as well as the random nature of some models being tested, classifier performance varied based on the random seed. Consequently, we'll measure performance as an average across 25 runs. Table 1 shows the cross-validation performance of each classifier on the genetic data after performing PCA transformation.

Recall that there are four labels: Severe Alzheimer’s, Moderate Alzheimer’s, Incipient (Mild) Alzheimer’s, and Control. So by chance, purely choosing labels at random, we should expect a score of around 0.25 assuming the underlying distribution of outcomes is unknown. Most of the models (except for Kernel SVM, which underperforms) are slightly better than chance, yet perform quite poorly overall. The best performing model was **Naive Bayes**, followed by AdaBoost, and then Logistic Regression. Naive Bayes and Logistic Regression are both parametric models, and AdaBoost is a mixture of parametric decision stumps. Non-parametric models like Kernel SVM, Random Forest, and KNN performed poorly. This is strong evidence against the initial hypothesis. Why was this so contrary to expectations? My guess is that non-parametric models may have overfit the data due to the small number of samples, and models might benefit from strong regularization in cases with very few samples.

![Multi-Label Classification Table](/images/alz/table1.png)

### Part 3: Binary Classification

Now let's try the same experiment again, with the modification that the target variable will be flattened into a binary classification. All patients with the disease, regardless of severity, are placed in a single category, while all control subjects are placed in another category. Note that in this case, there are only two labels, so a score of around 0.5 might be expected through random chance assuming the underlying distributions are unknown. We'll keep everything else the same. Table 2 shows the performance of each of the classifiers on this modified classification task.

![Binary Classification Table](/images/alz/table2.png)

Overall, classifier performance on this task shows improvement over the full multi-label classification task. All models outscore the baseline of 0.5. **Logistic Regression** performs the best, followed by Linear SVM and Kernel SVM. Kernel SVM is a non-linear, non-parametric model, but Logistic Regression and Linear SVM are both parametric models. Once again, the hypothesis that more non-linear, complex classifiers will outperform is smashed against the rocks of reality.

### Part 4: More Work To Do

A major challenge in this experiment was avoiding overfitting due to the extremely small sample size of the data. Non-parametric models, which were expected to outperform, suffered from overfitting on the small training set. However, data in the real world is often messy and sparse. Constructing a classifier that can work with very complex feature sets and small sample sizes is difficult, and requires techniques such as PCA; yet it can provide some predictive power in domains such as genomic data where analysis has historically been intractable.

As more genetic data is gathered for patients of Alzheimer’s disease, building a classifier pre diction model to predict Alzheimer’s risk should become easier. This should translate to improved diagnostic capabilities, and along with new treatments, will lead to better outcomes for those at risk.

If you want to dig more, or try it yourself, check out the code on my [Github](https://github.com/shaunish/alz)!